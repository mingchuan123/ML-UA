{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from gensim.models import word2vec\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning for the words in text\n",
    "def review_words(review,frequency):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #print(review_text)\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    letters = regex.sub(' ',review_text)\n",
    "    #print(letters)\n",
    "    words = letters.lower().split()\n",
    "    #print(words)\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    lemmatized_words = [lmtzr.lemmatize(w) for w in meaningful_words]\n",
    "    #print(lemmatized_words)\n",
    "    stemmed_words = [snowball.stem(w) for w in lemmatized_words]\n",
    "    output_words = ' '.join(stemmed_words)\n",
    "    \n",
    "    if frequency == False:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide reviews into sentences\n",
    "def review_sentence(review,tokenizer,frequency):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #print(len(raw_sentences))\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_words(raw_sentence,frequency))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract features from reviews \n",
    "def review_vector(file,frequency):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    data = pd.read_csv(file, header = 0, delimiter = '\\t', quoting =3)\n",
    "    raw_review = data['review']\n",
    "    size = len(data['review'])\n",
    "    review_all = []\n",
    "    for i in range(size):\n",
    "        review_clean = review_sentence(raw_review[i],tokenizer,frequency)\n",
    "        review_all += review_clean\n",
    "        if i%5000 == 0:\n",
    "            print('review set finished:{}'.format(i))\n",
    "            \n",
    "        \n",
    "    num_features = 200\n",
    "    min_word_count = 50\n",
    "    num_workers = 6\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "        \n",
    "    model = word2vec.Word2Vec(review_all, size = num_features, min_count = min_word_count,workers = num_workers, window = context, sample = downsampling)\n",
    "    model.init_sims(replace=True)\n",
    "    model_name = 'sentiment_vector'\n",
    "    model.save(model_name)    \n",
    "    \n",
    "    return review_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/zhaomingchuan/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:5000\n",
      "review set finished:10000\n",
      "review set finished:15000\n",
      "review set finished:20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:25000\n",
      "review set finished:30000\n",
      "review set finished:35000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:40000\n",
      "review set finished:45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review file finished\n"
     ]
    }
   ],
   "source": [
    "#Read data from unlabeledTrainData\n",
    "review_all = review_vector('unlabeledTrainData.tsv',False)\n",
    "data = word2vec.Word2Vec.load('sentiment_vector')\n",
    "\n",
    "df = pd.DataFrame(review_all)\n",
    "df.to_csv('words.csv',index=False,header=False)\n",
    "print('review file finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532928,)\n"
     ]
    }
   ],
   "source": [
    "review_all = np.array(review_all)\n",
    "print(review_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8360, 200)\n"
     ]
    }
   ],
   "source": [
    "#Feed word2vec model and extract features\n",
    "model_new = word2vec.Word2Vec.load('sentiment_vector')\n",
    "matrix = model_new.wv.syn0\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360\n"
     ]
    }
   ],
   "source": [
    "print(len(model_new.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.doesnt_match('man woman watch school money'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buck', 0.6363558769226074),\n",
       " ('dollar', 0.6173630952835083),\n",
       " ('cash', 0.579927921295166),\n",
       " ('fund', 0.5651991963386536),\n",
       " ('debt', 0.5647455453872681),\n",
       " ('fee', 0.552809476852417),\n",
       " ('profit', 0.5428676605224609),\n",
       " ('salari', 0.5420151352882385),\n",
       " ('ticket', 0.5404407382011414),\n",
       " ('expens', 0.5176376104354858)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.most_similar('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8360"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_new.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide reviews into sentences\n",
    "def review_bag(file,frequency):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    data = pd.read_csv(file, header = 0, delimiter = '\\t', quoting =3)\n",
    "    raw_review = data['review']\n",
    "    size = len(data['review'])\n",
    "    review_all = []\n",
    "    for i in range(size):\n",
    "        review_clean = review_sentence(raw_review[i],tokenizer,frequency)\n",
    "        review_all.append(review_clean)\n",
    "        if i%5000 == 0:\n",
    "            print('review set finished:{}'.format(i))\n",
    "    \n",
    "    return review_all        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find cluster center and form the features of the reviews\n",
    "def word_center(matrix,model,num_centers):\n",
    "    kmeans = KMeans(n_clusters=num_centers,random_state=0)\n",
    "    kmeans.fit(matrix)\n",
    "    centerid = kmeans.predict(matrix)\n",
    "    words = model.wv.index2word\n",
    "    wordscenter = dict(zip(words,centerid))\n",
    "    #print(wordscenter.keys())\n",
    "    return wordscenter\n",
    "\n",
    "\n",
    "def center_vector(words,wordscenter,num_centers):\n",
    "    vector = np.zeros(num_centers,dtype='float32')\n",
    "    words_new = [w.split() for w in words]\n",
    "    for item in words_new:\n",
    "        for word in item:\n",
    "            if word in wordscenter.keys():\n",
    "                index = wordscenter[word]\n",
    "                vector[index] += 1\n",
    "                #print(word)\n",
    "                #print(vector)\n",
    "    return vector\n",
    "\n",
    "def review_center(review,matrix,model):\n",
    "    review_center = []\n",
    "    num_centers = int(len(matrix)/10)\n",
    "    num_centers = 1000\n",
    "    wordscenter = word_center(matrix,model,num_centers)\n",
    "    for words in review:\n",
    "        result = center_vector(words,wordscenter,num_centers)\n",
    "        review_center.append(result)\n",
    "    return review_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/zhaomingchuan/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:5000\n",
      "review set finished:10000\n",
      "review set finished:15000\n",
      "review set finished:20000\n"
     ]
    }
   ],
   "source": [
    "#Read data from labeledTrainData\n",
    "reviews = review_bag('labeledTrainData.tsv',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find clusters and form features\n",
    "review_cluster = review_center(reviews,matrix,model_new)\n",
    "#print(review_cluster[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "review_cluster = np.array(review_cluster)\n",
    "print(review_cluster.shape)\n",
    "#print(review_cluster)\n",
    "review_new = normalize(review_cluster)\n",
    "#print(review_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read labels from labeledTrainData\n",
    "sentiment_p = pd.read_csv('labeledTrainData.tsv',header = 0, delimiter = '\\t', quoting =3)\n",
    "sentiment = sentiment_p['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(review_cluster, sentiment, test_size=0.4, random_state=0)\n",
    "svc = SVC(kernel = 'linear')\n",
    "svc.fit(X_train,y_train)\n",
    "#bayes = GaussianNB()\n",
    "#bayes = bayes.fit(X_train,y_train)\n",
    "pred = svc.predict(X_validate)\n",
    "score = accuracy_score(y_validate,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feed models\n",
    "#forest = RandomForestClassifier(n_estimators = 100)\n",
    "#forest = forest.fit(review_cluster,sentiment)\n",
    "bayes = GaussianNB()\n",
    "bayes = bayes.fit(review_cluster,sentiment)\n",
    "#SVM= SVC(kernel='linear')\n",
    "#SVM = SVM.fit(review_cluster,sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/zhaomingchuan/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaomingchuan/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review set finished:5000\n",
      "review set finished:10000\n",
      "review set finished:15000\n",
      "review set finished:20000\n"
     ]
    }
   ],
   "source": [
    "#Read test data and extract features\n",
    "test = review_bag('testData.tsv',True)\n",
    "test_cluster = review_center(test,matrix,model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_new = normalize(test_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "k = pd.read_csv('testData.tsv',header = 0, delimiter = '\\t', quoting =3)\n",
    "result = bayes.predict(test_cluster)\n",
    "output = pd.DataFrame({'id':k['id'],'sentiment':result})\n",
    "output.to_csv('Bag_of_Words_model_l.csv', index=False,quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
